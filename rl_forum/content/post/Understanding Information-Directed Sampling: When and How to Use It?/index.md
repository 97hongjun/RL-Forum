---
title: Understanding Information-Directed Sampling, When and How to Use It?
description: Botao Hao
date: 2022-11-09
publishdate: 2022-10-31
slug: understand_ids
image: info_ratio.png
categories:
    - Deepmind
    - Upcoming
---

### Logistics
<p>
    <strong> Time:</strong> 4:00-5:00 PM; Wednesday 11/09/2022<br>
    Hybrid Lecture <br>
    <strong>Locations:</strong> Packard 202 <a href="https://stanford.zoom.us/j/96783326250?pwd=NG9JS0I3U2psVXQ3SXNVSEFCK3V6UT09" target="_blank" rel="noopener noreferrer">Zoom Link</a>
    <br><br>
</p>

### Presenter
<p>
    Botao Hao<br>
    Research Scientist,<br>
    Deepmind, Mountain View<br>
</p>

### Abstract
<p>
    Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning. However, when and how to use this design principle in the right way remains open. I will discuss two questions: 1. When can IDS outperform optimism-based algorithms? 2. What is the right form of information ratio to optimize for reinforcement learning? To answer the first question, I will use sparse linear bandits as a showcase and prove that IDS can optimally address the information-regret trade-off while UCB and Thompson sampling fail. To answer the second question, I will derive prior-free Bayesian regret bounds for vanilla-IDS that maximizes the ratio form of the information ratio. Furthermore, I will discuss a computationally-efficient regularized-IDS that maximizes an additive form of the information ratio and show that it enjoys the same regret bound as vanilla-IDS.
</p>

### Reference
<p>
</p>

### Bio
<p>
    Botao Hao is a research scientist at Deepmind. Previously, he was a postdoc in the Department of Electrical Engineering at Princeton University. He received his Ph.D. from the Department of Statistics at Purdue University. 
</p>

### Recording
<p>
    <a href="https://www.youtube.com/watch?v=YrgOFV7fPcE" target="_blank" rel="noopener noreferrer">Lecture Recording</a><br>
</p>
