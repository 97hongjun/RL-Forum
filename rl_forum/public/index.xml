<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Stanford RL Forum</title>
        <link>https://97hongjun.github.io/</link>
        <description>Recent content on Stanford RL Forum</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 23 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://97hongjun.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Challenges in Scalable Training Data Attribution</title>
        <link>https://97hongjun.github.io/p/data-attribution/</link>
        <pubDate>Tue, 23 Apr 2024 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/data-attribution/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/data-attribution/TDA_cover.svg" alt="Featured image of post Challenges in Scalable Training Data Attribution" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:00-5:00 PM; 04/23/2024&lt;br&gt;
    &lt;strong&gt; Location: Allen 101X&lt;/strong&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Roger Grosse&lt;br&gt;
    Associate Professor,&lt;br&gt;
    Computer Science Department,&lt;br&gt;
    University of Toronto&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    How can we trace surprising behaviors of machine learning models back to their training data?  Influence functions and related methods aim to predict how the trained model would change if a specific training example were added or removed. Two issues have blocked their applicability to large neural nets: the difficulty of computing with neural net Hessians, and the inability of influence functions to capture implicit bias of optimizers. To address both questions, we reformulate training data attribution in terms of differentiating through the training procedure and present a scalable algorithm for approximating this higher-order derivative. This opens up the possibility of training data attribution in multi-stage training settings such as continual learning or foundation models.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Roger Grosse is an Associate Professor of Computer Science at the University of Toronto, and a founding member of the Vector Institute for Artificial Intelligence. His research focuses on using our understanding of deep learning to improve the safety and alignment of AI systems. He has held the Sloan Research Fellowship, CIFAR Canada AI Chair, and Canada Research Chair. Since 2022, he has also been a Member of Technical Staff on the Alignment Team at Anthropic.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;</description>
        </item>
        <item>
        <title>Recent Advances in Average-Reward Restless Bandits</title>
        <link>https://97hongjun.github.io/p/restless-bandit/</link>
        <pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/restless-bandit/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/restless-bandit/weina.svg" alt="Featured image of post Recent Advances in Average-Reward Restless Bandits" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:00-5:00 PM; 01/09/2024&lt;br&gt;
    &lt;strong&gt; Location: Packard 202&lt;/strong&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Weina Wang&lt;br&gt;
    Assistant Professor,&lt;br&gt;
    Computer Science Department,&lt;br&gt;
    Carnegie Mellon University&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    We consider the infinite-horizon, average reward restless bandit problem.  A central challenge is designing computationally efficient policies that achieve a diminishing optimality gap as the number of arms, N, grows large.  Existing policies, including the renowned Whittle index policy, all rely on a uniform global attractor property (UGAP) assumption to achieve asymptotic optimality, which is a complex and difficult-to-verify assumption.  In this talk, I will present new, sampling-based policy designs for restless bandits.  One of our proposed policies breaks the long-standing UGAP assumption for the first time, and the subsequent policies eliminate the need for the UGAP assumption to achieve asymptotic optimality.  Our techniques offer new insights into guaranteeing convergence (avoiding undesirable attractors or cycles) in large stochastic systems.  This talk is based on joint work with Yige Hong (CMU), Qiaomin Xie (UW–Madison), and Yudong Chen (UW–Madison).
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Weina Wang is an Assistant Professor in the Computer Science Department at Carnegie Mellon University. Her research lies in the broad area of applied probability and stochastic systems, with applications in cloud computing, data centers, and privacy-preserving data analytics. She was a joint postdoctoral research associate in the Coordinated Science Lab at the University of Illinois at Urbana-Champaign, and in the School of ECEE at Arizona State University, from 2016 to 2018. She received her Ph.D. degree in Electrical Engineering from Arizona State University in 2016, and her Bachelor’s degree from the Department of Electronic Engineering at Tsinghua University in 2009. Her dissertation received the Dean’s Dissertation Award in the Ira A. Fulton Schools of Engineering at Arizona State University in 2016. She received the Kenneth C. Sevcik Outstanding Student Paper Award at ACM SIGMETRICS 2016, the Best Paper Award at ACM MobiHoc 2022, an NSF CAREER award in 2022, and the ACM SIGMETRICS Rising Star Research Award in 2023.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;</description>
        </item>
        <item>
        <title>Continual Subtask Learning</title>
        <link>https://97hongjun.github.io/p/continual-subtask-learning/</link>
        <pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/continual-subtask-learning/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/continual-subtask-learning/adam_white.svg" alt="Featured image of post Continual Subtask Learning" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:00-5:00 PM; Wednesday 12/06/2023&lt;br&gt;
    &lt;strong&gt;Location:&lt;/strong&gt; Packard 202 &lt;br&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Adam White&lt;br&gt;
    Assistant Professor,&lt;br&gt;
    Unversity of Alberta&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    In many real-world problems the agent is much smaller than the vast world in which it must operate. In such scenarios, the world appears non-stationary to the agent, and thus we require agents capable of stable, non-convergent, never-ending learning. Successful agents must balance specializing their learning to the current situation with the need to learn many things over time which can be combined to learn yet new things--a concept known as scaffolding. We will start with an empirical study in Atari demonstrating how current deep reinforcement learning agents fail to learn continually, and in fact catastrophically unlearn over time. The remainder of the talk will focus on new architectures and algorithms for learning many things--subtasks formulated as general value functions--in parallel from a single stream of experience.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2303.07507&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Adam White is an Assistant Professor at the University of Alberta. He is a Fellow and Director of the Alberta Machine Intelligence Institute and a Principle Investigator of Reinforcement Learning and Artificial Intelligence group at the University of Alberta. Adam is a Canada CIFAR Chair in Artificial Intelligence. Adam is the co-creator of the Reinforcement Learning Specialization on Coursera. Adam&#39;s research is focused on understanding the fundamental principles of learning in young humans, animals, and artificial agents in both simulated worlds and real industrial control applications. Adam&#39;s group is deeply passionate about good empirical practices and new methodologies to help determine if our algorithms are ready for deployment in the real world.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Reinforcement Learning from Static Datasets Algorithms, Analysis and Applications</title>
        <link>https://97hongjun.github.io/p/rl-static-data/</link>
        <pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/rl-static-data/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/rl-static-data/Aviral.png" alt="Featured image of post Reinforcement Learning from Static Datasets Algorithms, Analysis and Applications" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:30-5:30 PM; 4/26/2023&lt;br&gt;
    &lt;strong&gt; Location: Building 200, Room 305&lt;/strong&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Aviral Kumar&lt;br&gt;
    PhD Student,&lt;br&gt;
    UC Berkeley&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    Typically, reinforcement learning (RL) methods rely on trial-and-error interaction with the environment from scratch to discover effective behaviors. While this sort of paradigm has the potential to discover good strategies, this paradigm also inhibits RL methods from collecting enough experience or training data in real-world problems where active interaction is expensive (e.g., in drug design) or dangerous (e.g., for robots operating around humans). My work develops approaches to alleviate this limitation: how can we learn policies to effectively make decisions entirely from previously-collected, static datasets in an offline manner? In this talk, I will discuss challenges that appear in this kind of offline reinforcement learning (offline RL) and develop algorithms and techniques to address these challenges. I will then discuss how my approaches for offline RL and decision-making have enabled us to make progress in real-world problems such as hardware accelerator design, robotic manipulation, and computational chemistry. Finally, I will discuss how we can start to enable offline RL methods to benefit from generalization capabilities offered by large and expressive models by understanding the interplay of these methods with gradient-based optimization. 
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Aviral Kumar is a final year Ph.D. candidate at UC Berkeley. His research focuses on developing effective and reliable approaches for (sequential) decision-making. Towards this goal, he focuses on designing reinforcement learning techniques to static datasets and on understanding and applying these methods in practice. Before his Ph.D., Aviral obtained his B.Tech. in Computer Science from IIT Bombay in India. He is a recipient of the C.V. &amp; Daulat Ramamoorthy Distinguished Research Award, awarded to 1 PhD student in Berkeley EECS for outstanding contributions to a new area of research in computer science, Facebook Ph.D. Fellowship in Machine Learning and Apple Scholars in AI/ML Ph.D. Fellowship.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=H7_RTFv7dDw&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;</description>
        </item>
        <item>
        <title>Insights into Intelligence</title>
        <link>https://97hongjun.github.io/p/insights-intelligence/</link>
        <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/insights-intelligence/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/insights-intelligence/sutton_banner.svg" alt="Featured image of post Insights into Intelligence" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 3:10-4:10 PM; Friday 3/17/2023&lt;br&gt;
    &lt;strong&gt;Location:&lt;/strong&gt; Hewlet 201 &lt;br&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Rich Sutton&lt;br&gt;
    Professor,&lt;br&gt;
    Unversity of Alberta&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    For thousands of years, great philosophers, and later, great scientists, have sought to understand intelligence and thereby better understand themselves. As Richard Feynman said “What I cannot create, I do not understand”, so now as we have begun to create intelligence in machines, we might expect to have gained a greater understanding of it. Has this happened? When I step back, looking past the hype and noise, I do feel that I have gained significant insights into intelligence, summarized below in bullet points. Some of these you may see as obviously true, others as unproven, and still others as simply wrong. Some are not claims at all, but simply definitions (though still arguably insights). I offer them in the spirit of openness and provocation:
    &lt;ul style=&#34;margin-left: 40px;&#34;&gt;
        &lt;li&gt;Intelligence is not the ability to mimic people, but rather the ability to achieve goals&lt;/li&gt;
        &lt;li&gt;Goals are well formulated as maximizing an externally received number (reward)&lt;/li&gt;
        &lt;li&gt;Intelligence is the domain-independent part of the ability to achieve goals&lt;/li&gt;
        &lt;li&gt;The world is generally much bigger and more complex than the intelligent agent&lt;/li&gt;
        &lt;li&gt;Accordingly, the agent’s computational resources, however great, are never enough&lt;/li&gt;
        &lt;li&gt;The above properties of the problem of intelligence require solutions in which the agent is divided into four parts, all learned: perception (state construction), reactive policy, value function, and state-transition model. Similar separations appear in many disparate fields&lt;/li&gt;
        &lt;li&gt;The agent further shapes its higher-level cognition by posing subproblems for itself&lt;/li&gt;
        &lt;li&gt;Off-policy learning and temporal abstraction are essential ambitions&lt;/li&gt;
    &lt;/ul&gt;
    In this talk I will explain some of these purported insights and assess their accuracy, significance, and implications.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Richard S. Sutton is a professor in the Department of Computing Science at the University of Alberta and a fellow of the Royal Society of London, the Royal Society of Canada, the Association for the Advancement of Artificial Intelligence, the Alberta Machine Intelligence Institute (Amii), and CIFAR. He received a PhD in computer science from the University of Massachusetts in 1984 and a BA in psychology from Stanford University in 1978. Prior to joining the University of Alberta in 2003, he worked in industry at AT&amp;T Labs and GTE Labs, and in academia at the University of Massachusetts. He helped found DeepMind Alberta in 2017 and worked there until its dissolution in 2023. At the University of Alberta, Sutton founded the Reinforcement Learning and Artificial Intelligence Lab, which now consists of ten principal investigators and about 100 people altogether.
    Sutton&#39;s research interests center on the learning problems facing a decision-making agent interacting with its environment, which he sees as central to intelligence. He has additional interests in animal learning psychology, in connectionist networks, and generally in systems that continually improve their representations and models of the world. He is co-author of the textbook Reinforcement Learning: An Introduction. His scientific publications have been cited more than 100,000 times. He is also a libertarian, a chess player, and a cancer survivor.&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Understanding Information-Directed Sampling, When and How to Use It?</title>
        <link>https://97hongjun.github.io/p/understand_ids/</link>
        <pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/understand_ids/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/understand_ids/info_ratio.png" alt="Featured image of post Understanding Information-Directed Sampling, When and How to Use It?" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:00-5:00 PM; Wednesday 11/09/2022&lt;br&gt;
    Hybrid Lecture &lt;br&gt;
    &lt;strong&gt;Locations:&lt;/strong&gt; Packard 202 &lt;a href=&#34;https://stanford.zoom.us/j/96783326250?pwd=NG9JS0I3U2psVXQ3SXNVSEFCK3V6UT09&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Zoom Link&lt;/a&gt;
    &lt;br&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Botao Hao&lt;br&gt;
    Research Scientist,&lt;br&gt;
    Deepmind, Mountain View&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning. However, when and how to use this design principle in the right way remains open. I will discuss two questions: 1. When can IDS outperform optimism-based algorithms? 2. What is the right form of information ratio to optimize for reinforcement learning? To answer the first question, I will use sparse linear bandits as a showcase and prove that IDS can optimally address the information-regret trade-off while UCB and Thompson sampling fail. To answer the second question, I will derive prior-free Bayesian regret bounds for vanilla-IDS that maximizes the ratio form of the information ratio. Furthermore, I will discuss a computationally-efficient regularized-IDS that maximizes an additive form of the information ratio and show that it enjoys the same regret bound as vanilla-IDS.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Botao Hao is a research scientist at Deepmind. Previously, he was a postdoc in the Department of Electrical Engineering at Princeton University. He received his Ph.D. from the Department of Statistics at Purdue University. 
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://www.youtube.com/watch?v=YrgOFV7fPcE&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Towards Instance-Optimal Algorithms for Reinforcement Learning</title>
        <link>https://97hongjun.github.io/p/instance-optimal-rl/</link>
        <pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/instance-optimal-rl/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/instance-optimal-rl/iorl.png" alt="Featured image of post Towards Instance-Optimal Algorithms for Reinforcement Learning" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:00-5:00 PM; Thursday 10/27/2022&lt;br&gt;
    Hybrid Lecture &lt;br&gt;
    &lt;strong&gt;Locations:&lt;/strong&gt; Packard 101 &lt;a href=&#34;https://stanford.zoom.us/j/95905740625?pwd=dHcyVzlUems0MzVuZ0N6emYzOGl6UT09&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Zoom Link&lt;/a&gt;
    &lt;br&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Kevin Jamieson&lt;br&gt;
    Assistant Professor,&lt;br&gt;
    University of Washington&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    The theory of reinforcement learning has focused on two fundamental problems: achieving low regret, and identifying epsilon-optimal policies. While in multi-armed bandits there exists a single algorithm that is instance-optimal for both, I will show in this talk that for tabular MDPs this is no longer possible—there exists a fundamental tradeoff between achieving low regret and identifying an epsilon-optimal policy at the instance-optimal rate. That is, popular algorithms that exploit optimism cannot be instance optimal. I will then present an algorithm that achieves the best known instance-dependent sample complexity for PAC tabular reinforcement learning which explicitly accounts for the sub-optimality gaps and attainable state visitation distributions in the underlying MDP. I will then discuss our recent work in the more general linear MDP setting where we have proposed an algorithm that is qualitatively very different but nevertheless achieves an instance-dependent sample complexity.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Kevin Jamieson is an Assistant Professor in the Paul G. Allen School of Computer Science &amp; Engineering at the University of Washington. His research explores how to leverage already-collected data to inform what future measurements to make next, in a closed loop. Jamieson has shown that such active learning can substantially reduce the sample complexity of learning in scenarios like multi-armed bandits, reinforcement learning, regression, and multi-class classification. He received his Ph.D. from the University of Wisconsin - Madison under the advisement of Robert Nowak, and was a post-doctoral researcher at UC Berkeley with Benjamin Recht. Jamieson&#39;s work has been recognized by an NSF CAREER award and Amazon Faculty Research award.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://www.youtube.com/watch?v=8hTBRD_k5wo&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Epistemic Neural Networks</title>
        <link>https://97hongjun.github.io/p/epistemic-neural-networks/</link>
        <pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/epistemic-neural-networks/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/epistemic-neural-networks/enn.jpeg" alt="Featured image of post Epistemic Neural Networks" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:00-5:00 PM; Tuesday 5/31/2022&lt;br&gt;
    Hybrid Lecture &lt;br&gt;
    &lt;strong&gt;Locations:&lt;/strong&gt; Packard 101, &lt;a href=&#34;https://stanford.zoom.us/meeting/register/tJwpcu6pqzMiEtUPHhPPm18qyByaf88cHF0_&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Zoom Link&lt;/a&gt;&lt;br&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Ian Osband&lt;br&gt;
    Research Scientist,&lt;br&gt;
    DeepMind, Mountain View&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    Effective decision, exploration, and adaptation often require an agent to know what it knows and, also, what it does not know.
    This capability relies on the quality of joint predictions of labels assigned to multiple inputs.  
    Conventional neural networks lack this capability and, since most research has focused on marginal predictions, this shortcoming has been largely overlooked.
    By assessing the quality of joint predictions it is possible to determine whether a neural network effectively distinguishes between epistemic uncertainty (that due to lack of knowledge) and aleatoric uncertainty (that due to chance).
    We introduce the epistemic neural network (ENN) as a general interface for uncertainty modeling in deep learning.
    While prior approaches to uncertainty modeling can be viewed as ENNs, the new interface facilitates comparison of joint predictions, and the design of novel architectures and algorithms.
    In particular, we introduce the epinet: an architecture that can supplement any existing neural network, including pretrained models and be trained with modest incremental computation to represent uncertainty.
    With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation.
    We demonstrate this efficacy across synthetic data, ImageNet, and sequential decision problems.
    As part of this effort we open-source experiment code.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2107.08924&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Ian is a research scientist at DeepMind in Mountain View, working on the design of efficient agents.
    That means artificial intelligence systems that can learn to perform well in decision problems with a reasonable amount of experience and/or compute.
    Before DeepMind, Ian completed his PhD at Stanford under the guidance of Benjamin Van Roy, studied Maths at Oxford, and had a brief spell working at JPMorgan in credit derivatives.
    For more information you can check out his website &lt;a href=&#34;http://iosband.github.io&#34; target=&#34;_blank&#34; &gt;http://iosband.github.io &lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://www.youtube.com/watch?v=j8an0dKcX4A&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Optimal Clustering with Bandit Feedback</title>
        <link>https://97hongjun.github.io/p/optimal-clustering/</link>
        <pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/optimal-clustering/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/optimal-clustering/optimalClustering.png" alt="Featured image of post Optimal Clustering with Bandit Feedback" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 2:00-3:00 PM; 5/24/2022&lt;br&gt;
    &lt;a href=&#34;https://stanford.zoom.us/meeting/register/tJwvf-qhqD8qG9wlCniUWI1YaILdFqRhU4Xn&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Zoom Link&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Vincent Y. F. Tan&lt;br&gt;
    Associate Professor and Dean&#39;s Chair&lt;br&gt;
    Deparrtment of Electrical and Computer Engineering&lt;br&gt;
    Department of Mathematics&lt;br&gt;
    National University of Singapore&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    This paper considers the problem of online clustering with bandit feedback. A set of arms (or items) can be partitioned into various groups that are unknown. Within each group, the observations associated to each of the arms follow the same distribution with the same mean vector. At each time step, the agent queries or pulls an arm and obtains an independent observation from the distribution it is associated to. Subsequent pulls depend on previous ones as well as the previously obtained samples. The agent&#39;s task is to uncover the underlying partition of the arms with the least number of arm pulls and with a probability of error not exceeding a prescribed constant δ. The problem proposed finds numerous applications from clustering of variants of viruses to online market segmentation. We present an instance-dependent information-theoretic lower bound on the expected sample complexity for this task, and design a computationally efficient and asymptotically optimal algorithm, namely Bandit Online Clustering (BOC). The algorithm includes a novel stopping rule for adaptive sequential testing that circumvents the need to exactly solve any NP-hard weighted clustering problem as its subroutines. We show through extensive simulations on synthetic and real-world datasets that BOC&#39;s performance matches the lower bound asymptotically, and significantly outperforms a non-adaptive baseline algorithm.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2202.04294&lt;/a&gt;
&lt;p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Vincent Y. F. Tan received the B.A. and M.Eng. degrees in electrical and information science from Cambridge University in 2005, and the Ph.D. degree in electrical engineering and computer science (EECS) from the Massachusetts Institute of Technology (MIT) in 2011. He is currently a Dean’s Chair Associate Professor with the Department of Electrical and Computer Engineering and the Department of Mathematics, National University of Singapore (NUS). His research interests include information theory, machine learning, and statistical signal processing.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://www.youtube.com/watch?v=2ALpGsAE2JE&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;</description>
        </item>
        <item>
        <title>Adaptivity and Confounding in Multi-Armed Bandit Experiments</title>
        <link>https://97hongjun.github.io/p/adaptive-confounding-bandit/</link>
        <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/adaptive-confounding-bandit/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/adaptive-confounding-bandit/adaptConfoundMAB.png" alt="Featured image of post Adaptivity and Confounding in Multi-Armed Bandit Experiments" /&gt;&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;
    &lt;strong&gt; Time:&lt;/strong&gt; 4:00-5:00 PM; 3/3/2022&lt;br&gt;
    &lt;strong&gt; Zoom Link: &lt;/strong&gt; &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://stanford.zoom.us/meeting/register/tJckfuCurzkvEtKKOBvDCrPv3McapgP6HygJ&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Daniel Russo&lt;br&gt;
    Associate Professor of Decision, Risk, and Operations division of Columbia Business School,&lt;br&gt;
    Columbia Business School&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    Multi-armed bandit algorithms minimize experimentation costs required to converge on optimal behavior. They do so by rapidly adapting experimentation effort away from poorly performing actions as feedback is observed. But this desirable feature makes them sensitive to confounding, which is the primary concern underlying classical randomized controlled trials. We highlight, for instance, that popular bandit algorithms cannot address the problem of identifying the best action when day-of-week effects may influence reward observations. In response, this paper proposes deconfounded Thompson sampling, which makes simple, but critical, modifications to the way Thompson sampling is usually applied. Theoretical guarantees suggest the algorithm strikes a delicate balance between adaptivity and robustness to confounding. It attains asymptotic lower bounds on the number of samples required to confidently identify the best action -- suggesting optimal adaptivity -- but also satisfies strong performance guarantees in the presence of day-of-week effects and delayed observations -- suggesting unusual robustness. At the core of the paper is a new model of contextual bandit experiments in which issues of delayed learning and distribution shift arise organically.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2202.09036.pdf&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Daniel Russo is an Associate Professor in the Decision, Risk, and Operations division of Columbia Business School. His research focuses on problems at the intersection of sequential decision-making and statistical machine learning. He completed his PhD at Stanford under the supervision of Ben Van Roy.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://www.youtube.com/watch?v=h1yAaJzA5Ao&amp;list=PLv_7iO_xlL0Ks_rnHPbzHaIOHfkOu_hfw&amp;index=44&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;</description>
        </item>
        <item>
        <title>Reinforcement Learning, Bit by Bit</title>
        <link>https://97hongjun.github.io/p/rl-bit-by-bit/</link>
        <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/rl-bit-by-bit/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/rl-bit-by-bit/rlBitByBit.png" alt="Featured image of post Reinforcement Learning, Bit by Bit" /&gt;&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Xiuyuan (Lucy) Lu&lt;br&gt;
    Research Scientist,&lt;br&gt;
    DeepMind, Mountain View&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We develop concepts and establish a regret bound that together offer principled guidance. The bound sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that demonstrate improvements in data efficiency.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2103.04047.pdf&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Xiuyuan Lu is a research scientist at DeepMind. She is interested in building data-efficient reinforcement learning agents. She obtained her PhD in 2020 from the Department of Management Science and Engineering at Stanford University.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://stanford.zoom.us/rec/play/m9BP7430B8jcl6iFRomPWjcXfMzjSXihSXlOcYLCUWP-9oYexMWU1Xb8Sfu1LSyEX0196LsWcmS3dSmq.EloRFp80gdQZDq-1?continueMode=true&amp;_x_zm_rtaid=KV-j0mABQLqkJR5QUbWjSA.1618942041710.c4629d090448fa38d57f34556ef7427e&amp;_x_zm_rhtaid=389&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture 1 Recording&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;https://stanford.zoom.us/rec/play/kfMpDYfTWqnOZEe656ISqWhSAbyWiGUKZA1lAHliRCrqfhl_6biL3xH10YDkp3W9N-k_ElChym9EO8Nn.j8ylads97aC1izOZ?continueMode=true&amp;_x_zm_rtaid=3Yguu6A8QVK-2yxQVJAMSA.1635206758825.17a5ebec65255ebeea1580a6ac77476c&amp;_x_zm_rhtaid=487&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture 2 Recording&lt;/a&gt;
&lt;/p&gt;</description>
        </item>
        <item>
        <title>Provable Model-based Nonlinear Bandit and Reinforcement Learning</title>
        <link>https://97hongjun.github.io/p/provable-model-based-bandit-rl/</link>
        <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/provable-model-based-bandit-rl/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/provable-model-based-bandit-rl/modelBasedMABRL.png" alt="Featured image of post Provable Model-based Nonlinear Bandit and Reinforcement Learning" /&gt;&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Tengyu Ma&lt;br&gt;
    Assistant Professor of Computer Science and Statistics,&lt;br&gt;
    Stanford&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    Deep model-based reinforcement learning methods have achieved state-of-the-art sample efficiency but we lack a theoretical understanding of them. This talk will first show that convergence to a global maximum requires an exponential number of samples even for a one-layer neural net bandit problem, which is strictly easier than RL. Therefore, we propose to study convergence to local maxima. For both nonlinear bandit and RL, I will present a model-based algorithm, Virtual Ascent with Online Model Learner (ViOL), which provably converges to a local maximum with sample complexity that only depends on the sequential Rademacher complexity of the model class. Our results imply novel global or local regret bounds on several concrete settings such as linear bandit with finite or sparse model class, and two-layer neural net bandit.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2102.04168.pdf&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Tengyu Ma is an assistant professor of Computer Science and Statistics at Stanford University. He received his Ph.D. from Princeton University and B.E. from Tsinghua University. His research interests include topics in machine learning and algorithms, such as deep learning and its theory, non-convex optimization, deep reinforcement learning, representation learning, and high-dimensional statistics. He is a recipient of NIPS&#39;16 best student paper award, COLT&#39;18 best paper award, ACM Doctoral Dissertation Award Honorable Mention, and Sloan Fellowship.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://stanford.zoom.us/rec/play/iT85r9ttX7iI9YB9VF6yUDbXqF-wBOGRzJbrIKQG6MhFNP3KjFjEol2auX2wyCjRzcur5fFV9ypc1fBA.UAtgiQbKjj40Pw6l?continueMode=true&amp;_x_zm_rtaid=ZP9lLdXYQNKYXQgXq7aQrQ.1618800579957.5a319de73a91a250c868e79e52750756&amp;_x_zm_rhtaid=796&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion Asymptotics for Sequential Experiments</title>
        <link>https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/</link>
        <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/diffusionAsymptotics.png" alt="Featured image of post Diffusion Asymptotics for Sequential Experiments" /&gt;&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Kuang Xu&lt;br&gt;
    Associate Professor of Operations, Information and Technology,&lt;br&gt;
    Stanford Graduate School of Business&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    I will discuss in this talk a new diffusion-asymptotic analysis for sequentially randomized experiments. Rather than taking sample size n to infinity while keeping the problem parameters fixed, we let the mean signal level scale to the order 1/\sqrt{n} so as to preserve the difficulty of the learning task as n gets large. In this regime, we show that the behavior of a class of methods for sequential experimentation converges to a diffusion limit. This connection enables us to make sharp performance predictions and obtain new insights on the behavior of Thompson sampling. Our diffusion asymptotics also help resolve a discrepancy between the Θ(log(n)) regret predicted by the fixed-parameter, large-sample asymptotics on the one hand, and the Θ(\sqrt{n}) regret from worst-case, finite-sample analysis on the other, suggesting that it is an appropriate asymptotic regime for understanding practical large-scale sequential experiments.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2101.09855.pdf&lt;/a&gt;
&lt;/p&gt;
&lt;h3 id=&#34;bio&#34;&gt;Bio&lt;/h3&gt;
&lt;p&gt;
    Kuang Xu is an Associate Professor of Operations, Information and Technology at Stanford Graduate School of Business, and Associate Professor by courtesy with the Electrical Engineering Department, Stanford University. Born in Suzhou, China, he received the B.S. degree in Electrical Engineering (2009) from the University of Illinois at Urbana-Champaign, and the Ph.D. degree in Electrical Engineering and Computer Science (2014) from the Massachusetts Institute of Technology. His research primarily focuses on understanding fundamental properties and design principles of large-scale stochastic systems using tools from probability theory and optimization, with applications in queueing networks, healthcare, privacy and machine learning. He received First Place in the INFORMS George E. Nicholson Student Paper Competition (2011), the Best Paper Award, as well as the Kenneth C. Sevcik Outstanding Student Paper Award at ACM SIGMETRICS (2013), and the ACM SIGMETRICS Rising Star Research Award (2020). He currently serves as an Associate Editor for Operations Research.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://stanford.zoom.us/rec/play/3H4nNPjycniAayUSk5o6h3z34jO4Qbup411T_xKqxQVTtcBnNs5oA2-h3hwjD0Q2t7sM2nLdtBwEsBRi.Wt5CMVlrHws0D9O3?continueMode=true&amp;_x_zm_rtaid=udQo9uzHR6WuPphcmSqFfg.1635206852126.aab3b197e3cd6df4cab5b980c64072fb&amp;_x_zm_rhtaid=736&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Lectures on Information Directed Sampling</title>
        <link>https://97hongjun.github.io/p/lec-ids/</link>
        <pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate>
        
        <guid>https://97hongjun.github.io/p/lec-ids/</guid>
        <description>&lt;img src="https://97hongjun.github.io/p/lec-ids/lecturesIDS.png" alt="Featured image of post Lectures on Information Directed Sampling" /&gt;&lt;h3 id=&#34;presenter&#34;&gt;Presenter&lt;/h3&gt;
&lt;p&gt;
    Tor Lattimore&lt;br&gt;
    Research Scientist,&lt;br&gt;
    Deepmind, London&lt;br&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;
    Tor will give a whirlwind tour of a series of recent papers on the information directed sampling algorithm for sequential decision-making. The results come in three flavours. First, generalising and applying the IDS algorithm to problems with a rich information structure such as convex bandits and partial monitoring. Second, showing a connection between the optimisation problem solved by IDS and the optimisation problem that determines the asymptotic lower bound for stochastic structured bandit problems. Third, showing a deep connection between IDS and the mirror descent framework for convex optimisation.
&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2011.05944&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2009.12228&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/1907.05772&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2006.00475&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2002.11182&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/1902.00470&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;url&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/1905.11817&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
### Bio
&lt;p&gt;
    Tor Lattimore is a research scientist at DeepMind working on the foundations of machine learning and especially decision-making. Before joining DeepMind he was an assistant professor at Indiana University and a postdoc at the University of Alberta. He obtained his PhD from the Australian National University under the supervision of Marcus Hutter in 2014.
&lt;/p&gt;
&lt;h3 id=&#34;recording&#34;&gt;Recording&lt;/h3&gt;
&lt;p&gt;
    &lt;a href=&#34;https://stanford.zoom.us/rec/play/sLK9KgHvSbo3j2ccznm0wUj1rU1KWG_9ck0X5SRMcVUpD5iqLckJx_Y8RE93lboxzsAuPBd57iXojZPb.qM74pXvpYsyR1Tq7?continueMode=true&amp;_x_zm_rtaid=1qmp5vKrRK-Cc05aVJxWfQ.1610393820266.44dec0e3d84a783b1da06cb3f008edfc&amp;_x_zm_rhtaid=823&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture 1 Recording&lt;/a&gt;&lt;br&gt;
    &lt;a href=&#34;https://stanford.zoom.us/rec/play/a7MMBIP4KJzSG6srg8rCsv0X2pGLUJXIfVRmiqpIAORnY-mN3cY0NKxidbmA-myNzIC2mPcH0I_vdOYp.BGW7u0t-Vw0teSga?startTime=1610560965000&amp;_x_zm_rtaid=omqNQuvITKyU7pBn-xn8xg.1610693976189.6d9c899cb89e2fbd1a709c7cc59bd220&amp;_x_zm_rhtaid=346&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Lecture 2 Recording&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
