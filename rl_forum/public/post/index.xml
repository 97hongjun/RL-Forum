<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Stanford RL Forum</title>
    <link>https://97hongjun.github.io/post/</link>
    <description>Recent content in Posts on Stanford RL Forum</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://97hongjun.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Continual Subtask Learning</title>
      <link>https://97hongjun.github.io/p/continual-subtask-learning/</link>
      <pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/continual-subtask-learning/</guid>
      <description>Logistics   Time: 4:00-5:00 PM; Wednesday 12/06/2023
Location: Packard 202 Presenter  Adam White
Assistant Professor,
Unversity of Alberta
Abstract  In many real-world problems the agent is much smaller than the vast world in which it must operate. In such scenarios, the world appears non-stationary to the agent, and thus we require agents capable of stable, non-convergent, never-ending learning. Successful agents must balance specializing their learning to the current situation with the need to learn many things over time which can be combined to learn yet new things--a concept known as scaffolding.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning from Static Datasets Algorithms, Analysis and Applications</title>
      <link>https://97hongjun.github.io/p/rl-static-data/</link>
      <pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/rl-static-data/</guid>
      <description>Logistics   Time: 4:30-5:30 PM; 4/26/2022
 Location: Building 200, Room 305 Presenter  Aviral Kumar
PhD Student,
UC Berkeley
Abstract  Typically, reinforcement learning (RL) methods rely on trial-and-error interaction with the environment from scratch to discover effective behaviors. While this sort of paradigm has the potential to discover good strategies, this paradigm also inhibits RL methods from collecting enough experience or training data in real-world problems where active interaction is expensive (e.</description>
    </item>
    
    <item>
      <title>Insights into Intelligence</title>
      <link>https://97hongjun.github.io/p/insights-intelligence/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/insights-intelligence/</guid>
      <description>Logistics   Time: 3:10-4:10 PM; Friday 3/17/2023
Location: Hewlet 201 Presenter  Rich Sutton
Professor,
Unversity of Alberta
Abstract  For thousands of years, great philosophers, and later, great scientists, have sought to understand intelligence and thereby better understand themselves. As Richard Feynman said “What I cannot create, I do not understand”, so now as we have begun to create intelligence in machines, we might expect to have gained a greater understanding of it.</description>
    </item>
    
    <item>
      <title>Understanding Information-Directed Sampling, When and How to Use It?</title>
      <link>https://97hongjun.github.io/p/understand_ids/</link>
      <pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/understand_ids/</guid>
      <description>Logistics   Time: 4:00-5:00 PM; Wednesday 11/09/2022
Hybrid Lecture Locations: Packard 202 Zoom Link Presenter  Botao Hao
Research Scientist,
Deepmind, Mountain View
Abstract  Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning. However, when and how to use this design principle in the right way remains open. I will discuss two questions: 1. When can IDS outperform optimism-based algorithms? 2. What is the right form of information ratio to optimize for reinforcement learning?</description>
    </item>
    
    <item>
      <title>Towards Instance-Optimal Algorithms for Reinforcement Learning</title>
      <link>https://97hongjun.github.io/p/instance-optimal-rl/</link>
      <pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/instance-optimal-rl/</guid>
      <description>Logistics   Time: 4:00-5:00 PM; Thursday 10/27/2022
Hybrid Lecture Locations: Packard 101 Zoom Link Presenter  Kevin Jamieson
Assistant Professor,
University of Washington
Abstract  The theory of reinforcement learning has focused on two fundamental problems: achieving low regret, and identifying epsilon-optimal policies. While in multi-armed bandits there exists a single algorithm that is instance-optimal for both, I will show in this talk that for tabular MDPs this is no longer possible—there exists a fundamental tradeoff between achieving low regret and identifying an epsilon-optimal policy at the instance-optimal rate.</description>
    </item>
    
    <item>
      <title>Epistemic Neural Networks</title>
      <link>https://97hongjun.github.io/p/epistemic-neural-networks/</link>
      <pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/epistemic-neural-networks/</guid>
      <description>Logistics   Time: 4:00-5:00 PM; Tuesday 5/31/2022
Hybrid Lecture Locations: Packard 101, Zoom Link
Presenter  Ian Osband
Research Scientist,
DeepMind, Mountain View
Abstract  Effective decision, exploration, and adaptation often require an agent to know what it knows and, also, what it does not know. This capability relies on the quality of joint predictions of labels assigned to multiple inputs. Conventional neural networks lack this capability and, since most research has focused on marginal predictions, this shortcoming has been largely overlooked.</description>
    </item>
    
    <item>
      <title>Optimal Clustering with Bandit Feedback</title>
      <link>https://97hongjun.github.io/p/optimal-clustering/</link>
      <pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/optimal-clustering/</guid>
      <description>Logistics   Time: 2:00-3:00 PM; 5/24/2022
Zoom Link
Presenter  Vincent Y. F. Tan
Associate Professor and Dean&#39;s Chair
Deparrtment of Electrical and Computer Engineering
Department of Mathematics
National University of Singapore
Abstract  This paper considers the problem of online clustering with bandit feedback. A set of arms (or items) can be partitioned into various groups that are unknown. Within each group, the observations associated to each of the arms follow the same distribution with the same mean vector.</description>
    </item>
    
    <item>
      <title>Adaptivity and Confounding in Multi-Armed Bandit Experiments</title>
      <link>https://97hongjun.github.io/p/adaptive-confounding-bandit/</link>
      <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/adaptive-confounding-bandit/</guid>
      <description>Logistics   Time: 4:00-5:00 PM; 3/3/2022
 Zoom Link:  https://stanford.zoom.us/meeting/register/tJckfuCurzkvEtKKOBvDCrPv3McapgP6HygJ Presenter  Daniel Russo
Associate Professor of Decision, Risk, and Operations division of Columbia Business School,
Columbia Business School
Abstract  Multi-armed bandit algorithms minimize experimentation costs required to converge on optimal behavior. They do so by rapidly adapting experimentation effort away from poorly performing actions as feedback is observed. But this desirable feature makes them sensitive to confounding, which is the primary concern underlying classical randomized controlled trials.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning, Bit by Bit</title>
      <link>https://97hongjun.github.io/p/rl-bit-by-bit/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/rl-bit-by-bit/</guid>
      <description>Presenter  Xiuyuan (Lucy) Lu
Research Scientist,
DeepMind, Mountain View
Abstract  Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We develop concepts and establish a regret bound that together offer principled guidance. The bound sheds light on questions of what information to seek, how to seek that information, and what information to retain.</description>
    </item>
    
    <item>
      <title>Provable Model-based Nonlinear Bandit and Reinforcement Learning</title>
      <link>https://97hongjun.github.io/p/provable-model-based-bandit-rl/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/provable-model-based-bandit-rl/</guid>
      <description>Presenter  Tengyu Ma
Assistant Professor of Computer Science and Statistics,
Stanford
Abstract  Deep model-based reinforcement learning methods have achieved state-of-the-art sample efficiency but we lack a theoretical understanding of them. This talk will first show that convergence to a global maximum requires an exponential number of samples even for a one-layer neural net bandit problem, which is strictly easier than RL. Therefore, we propose to study convergence to local maxima.</description>
    </item>
    
    <item>
      <title>Diffusion Asymptotics for Sequential Experiments</title>
      <link>https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/</guid>
      <description>Presenter  Kuang Xu
Associate Professor of Operations, Information and Technology,
Stanford Graduate School of Business
Abstract  I will discuss in this talk a new diffusion-asymptotic analysis for sequentially randomized experiments. Rather than taking sample size n to infinity while keeping the problem parameters fixed, we let the mean signal level scale to the order 1/\sqrt{n} so as to preserve the difficulty of the learning task as n gets large.</description>
    </item>
    
    <item>
      <title>Lectures on Information Directed Sampling</title>
      <link>https://97hongjun.github.io/p/lec-ids/</link>
      <pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://97hongjun.github.io/p/lec-ids/</guid>
      <description>Presenter  Tor Lattimore
Research Scientist,
Deepmind, London
Abstract  Tor will give a whirlwind tour of a series of recent papers on the information directed sampling algorithm for sequential decision-making. The results come in three flavours. First, generalising and applying the IDS algorithm to problems with a rich information structure such as convex bandits and partial monitoring. Second, showing a connection between the optimisation problem solved by IDS and the optimisation problem that determines the asymptotic lower bound for stochastic structured bandit problems.</description>
    </item>
    
  </channel>
</rss>
