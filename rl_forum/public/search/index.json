[{"content":"Logistic   Time: 4:00-5:00 PM; 3/3/2022\n Zoom Link:  https://stanford.zoom.us/meeting/register/tJckfuCurzkvEtKKOBvDCrPv3McapgP6HygJ Presenter  Daniel Russo\nAssociate Professor of Decision, Risk, and Operations division of Columbia Business School,\nColumbia Business School\nAbstract  Multi-armed bandit algorithms minimize experimentation costs required to converge on optimal behavior. They do so by rapidly adapting experimentation effort away from poorly performing actions as feedback is observed. But this desirable feature makes them sensitive to confounding, which is the primary concern underlying classical randomized controlled trials. We highlight, for instance, that popular bandit algorithms cannot address the problem of identifying the best action when day-of-week effects may influence reward observations. In response, this paper proposes deconfounded Thompson sampling, which makes simple, but critical, modifications to the way Thompson sampling is usually applied. Theoretical guarantees suggest the algorithm strikes a delicate balance between adaptivity and robustness to confounding. It attains asymptotic lower bounds on the number of samples required to confidently identify the best action -- suggesting optimal adaptivity -- but also satisfies strong performance guarantees in the presence of day-of-week effects and delayed observations -- suggesting unusual robustness. At the core of the paper is a new model of contextual bandit experiments in which issues of delayed learning and distribution shift arise organically. Reference https://arxiv.org/pdf/2202.09036.pdf\nBio  Daniel Russo is an Associate Professor in the Decision, Risk, and Operations division of Columbia Business School. His research focuses on problems at the intersection of sequential decision-making and statistical machine learning. He completed his PhD at Stanford under the supervision of Ben Van Roy. Recording Lecture Recording\n","date":"2022-03-03T00:00:00Z","image":"https://97hongjun.github.io/p/adaptive-confounding-bandit/russo_hu3d03a01dcc18bc5be0e67db3d8d209a6_1454388_120x120_fill_q75_box_smart1.jpg","permalink":"https://97hongjun.github.io/p/adaptive-confounding-bandit/","title":"Adaptivity and Confounding in Multi-Armed Bandit Experiments"},{"content":"Presenter  Xiuyuan (Lucy) Lu\nResearch Scientist,\nDeepMind, Mountain View\nAbstract  Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We develop concepts and establish a regret bound that together offer principled guidance. The bound sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that demonstrate improvements in data efficiency. Reference https://arxiv.org/pdf/2103.04047.pdf\nBio  Xiuyuan Lu is a research scientist at DeepMind. She is interested in building data-efficient reinforcement learning agents. She obtained her PhD in 2020 from the Department of Management Science and Engineering at Stanford University. Recording  Lecture 1 Recording\nLecture 2 Recording ","date":"2021-04-20T00:00:00Z","image":"https://97hongjun.github.io/p/rl-bit-by-bit/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://97hongjun.github.io/p/rl-bit-by-bit/","title":"Reinforcement Learning, Bit by Bit"},{"content":"Presenter  Tengyu Ma\nAssistant Professor of Computer Science and Statistics,\nStanford\nAbstract  Deep model-based reinforcement learning methods have achieved state-of-the-art sample efficiency but we lack a theoretical understanding of them. This talk will first show that convergence to a global maximum requires an exponential number of samples even for a one-layer neural net bandit problem, which is strictly easier than RL. Therefore, we propose to study convergence to local maxima. For both nonlinear bandit and RL, I will present a model-based algorithm, Virtual Ascent with Online Model Learner (ViOL), which provably converges to a local maximum with sample complexity that only depends on the sequential Rademacher complexity of the model class. Our results imply novel global or local regret bounds on several concrete settings such as linear bandit with finite or sparse model class, and two-layer neural net bandit. Reference https://arxiv.org/pdf/2102.04168.pdf\nBio  Tengyu Ma is an assistant professor of Computer Science and Statistics at Stanford University. He received his Ph.D. from Princeton University and B.E. from Tsinghua University. His research interests include topics in machine learning and algorithms, such as deep learning and its theory, non-convex optimization, deep reinforcement learning, representation learning, and high-dimensional statistics. He is a recipient of NIPS'16 best student paper award, COLT'18 best paper award, ACM Doctoral Dissertation Award Honorable Mention, and Sloan Fellowship. Recording  Recording\n","date":"2021-04-15T00:00:00Z","image":"https://97hongjun.github.io/p/provable-model-based-bandit-rl/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://97hongjun.github.io/p/provable-model-based-bandit-rl/","title":"Provable Model-based Nonlinear Bandit and Reinforcement Learning"},{"content":"Presenter  Kuang Xu\nAssociate Professor of Operations, Information and Technology,\nStanford Graduate School of Business\nAbstract  I will discuss in this talk a new diffusion-asymptotic analysis for sequentially randomized experiments. Rather than taking sample size n to infinity while keeping the problem parameters fixed, we let the mean signal level scale to the order 1/\\sqrt{n} so as to preserve the difficulty of the learning task as n gets large. In this regime, we show that the behavior of a class of methods for sequential experimentation converges to a diffusion limit. This connection enables us to make sharp performance predictions and obtain new insights on the behavior of Thompson sampling. Our diffusion asymptotics also help resolve a discrepancy between the Θ(log(n)) regret predicted by the fixed-parameter, large-sample asymptotics on the one hand, and the Θ(\\sqrt{n}) regret from worst-case, finite-sample analysis on the other, suggesting that it is an appropriate asymptotic regime for understanding practical large-scale sequential experiments. Reference https://arxiv.org/pdf/2101.09855.pdf\nBio  Kuang Xu is an Associate Professor of Operations, Information and Technology at Stanford Graduate School of Business, and Associate Professor by courtesy with the Electrical Engineering Department, Stanford University. Born in Suzhou, China, he received the B.S. degree in Electrical Engineering (2009) from the University of Illinois at Urbana-Champaign, and the Ph.D. degree in Electrical Engineering and Computer Science (2014) from the Massachusetts Institute of Technology. His research primarily focuses on understanding fundamental properties and design principles of large-scale stochastic systems using tools from probability theory and optimization, with applications in queueing networks, healthcare, privacy and machine learning. He received First Place in the INFORMS George E. Nicholson Student Paper Competition (2011), the Best Paper Award, as well as the Kenneth C. Sevcik Outstanding Student Paper Award at ACM SIGMETRICS (2013), and the ACM SIGMETRICS Rising Star Research Award (2020). He currently serves as an Associate Editor for Operations Research. Recording  Recording\n","date":"2021-03-23T00:00:00Z","image":"https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/flower_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/","title":"Diffusion Asymptotics for Sequential Experiments"},{"content":"Presenter  Tor Lattimore\nResearch Scientist,\nDeepmind, London\nAbstract  Tor will give a whirlwind tour of a series of recent papers on the information directed sampling algorithm for sequential decision-making. The results come in three flavours. First, generalising and applying the IDS algorithm to problems with a rich information structure such as convex bandits and partial monitoring. Second, showing a connection between the optimisation problem solved by IDS and the optimisation problem that determines the asymptotic lower bound for stochastic structured bandit problems. Third, showing a deep connection between IDS and the mirror descent framework for convex optimisation. Reference  https://arxiv.org/abs/2011.05944\nhttps://arxiv.org/abs/2009.12228\nhttps://arxiv.org/abs/1907.05772\nhttps://arxiv.org/abs/2006.00475\nhttps://arxiv.org/abs/2002.11182\nhttps://arxiv.org/abs/1902.00470\nhttps://arxiv.org/abs/1905.11817\n### Bio  Tor Lattimore is a research scientist at DeepMind working on the foundations of machine learning and especially decision-making. Before joining DeepMind he was an assistant professor at Indiana University and a postdoc at the University of Alberta. He obtained his PhD from the Australian National University under the supervision of Marcus Hutter in 2014. Recording  Lecture 1 Recording\nLecture 2 Recording\n","date":"2021-01-11T00:00:00Z","image":"https://97hongjun.github.io/p/lec-ids/walk_hu958d513eeefe5556a31d065479ecc5ac_14205_120x120_fill_q75_box_smart1.jpg","permalink":"https://97hongjun.github.io/p/lec-ids/","title":"Lectures on Information Directed Sampling"}]