[{"content":"Logistics   Time: 4:00-5:00 PM; Tuesday 5/31/2022\nHybrid Lecture Locations: Packard 101, Zoom Link to be posted Presenter  Ian Osband\nResearch Scientist,\nDeepMind, Mountain View\nAbstract  Effective decision, exploration, and adaptation often require an agent to know what it knows and, also, what it does not know. This capability relies on the quality of joint predictions of labels assigned to multiple inputs. Conventional neural networks lack this capability and, since most research has focused on marginal predictions, this shortcoming has been largely overlooked. By assessing the quality of joint predictions it is possible to determine whether a neural network effectively distinguishes between epistemic uncertainty (that due to lack of knowledge) and aleatoric uncertainty (that due to chance). We introduce the epistemic neural network (ENN) as a general interface for uncertainty modeling in deep learning. While prior approaches to uncertainty modeling can be viewed as ENNs, the new interface facilitates comparison of joint predictions, and the design of novel architectures and algorithms. In particular, we introduce the epinet: an architecture that can supplement any existing neural network, including pretrained models and be trained with modest incremental computation to represent uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. We demonstrate this efficacy across synthetic data, ImageNet, and sequential decision problems. As part of this effort we open-source experiment code. Reference https://arxiv.org/abs/2107.08924\nBio  Ian is a research scientist at DeepMind in Mountain View, working on the design of efficient agents. That means artificial intelligence systems that can learn to perform well in decision problems with a reasonable amount of experience and/or compute. Before DeepMind, Ian completed his PhD at Stanford under the guidance of Benjamin Van Roy, studied Maths at Oxford, and had a brief spell working at JPMorgan in credit derivatives. For more information you can check out his website http://iosband.github.io Recording ","date":"2022-05-31T00:00:00Z","image":"https://97hongjun.github.io/p/epistemic-neural-networks/enn_hu6bd8b418fe28309e2affecc3501ed6d4_890549_120x120_fill_q75_box_smart1.jpeg","permalink":"https://97hongjun.github.io/p/epistemic-neural-networks/","title":"Epistemic Neural Networks"},{"content":"Logistics   Time: 2:00-3:00 PM; 5/24/2022\nZoom Lecture Presenter  Vincent Y. F. Tan\nAssociate Professor and Dean's Chair\nDeparrtment of Electrical and Computer Engineering\nDepartment of Mathematics\nNational University of Singapore\nAbstract  This paper considers the problem of online clustering with bandit feedback. A set of arms (or items) can be partitioned into various groups that are unknown. Within each group, the observations associated to each of the arms follow the same distribution with the same mean vector. At each time step, the agent queries or pulls an arm and obtains an independent observation from the distribution it is associated to. Subsequent pulls depend on previous ones as well as the previously obtained samples. The agent's task is to uncover the underlying partition of the arms with the least number of arm pulls and with a probability of error not exceeding a prescribed constant δ. The problem proposed finds numerous applications from clustering of variants of viruses to online market segmentation. We present an instance-dependent information-theoretic lower bound on the expected sample complexity for this task, and design a computationally efficient and asymptotically optimal algorithm, namely Bandit Online Clustering (BOC). The algorithm includes a novel stopping rule for adaptive sequential testing that circumvents the need to exactly solve any NP-hard weighted clustering problem as its subroutines. We show through extensive simulations on synthetic and real-world datasets that BOC's performance matches the lower bound asymptotically, and significantly outperforms a non-adaptive baseline algorithm. Reference https://arxiv.org/abs/2202.04294\nBio  Vincent Y. F. Tan received the B.A. and M.Eng. degrees in electrical and information science from Cambridge University in 2005, and the Ph.D. degree in electrical engineering and computer science (EECS) from the Massachusetts Institute of Technology (MIT) in 2011. He is currently a Dean’s Chair Associate Professor with the Department of Electrical and Computer Engineering and the Department of Mathematics, National University of Singapore (NUS). His research interests include information theory, machine learning, and statistical signal processing. Recording ","date":"2022-05-24T00:00:00Z","image":"https://97hongjun.github.io/p/optimal-clustering/optimalClustering_hue56d52807ea01bc504cdc0e6d3cda98b_37640_120x120_fill_box_smart1_3.png","permalink":"https://97hongjun.github.io/p/optimal-clustering/","title":"Optimal Clustering with Bandit Feedback"},{"content":"Logistics   Time: 4:00-5:00 PM; 3/3/2022\n Zoom Link:  https://stanford.zoom.us/meeting/register/tJckfuCurzkvEtKKOBvDCrPv3McapgP6HygJ Presenter  Daniel Russo\nAssociate Professor of Decision, Risk, and Operations division of Columbia Business School,\nColumbia Business School\nAbstract  Multi-armed bandit algorithms minimize experimentation costs required to converge on optimal behavior. They do so by rapidly adapting experimentation effort away from poorly performing actions as feedback is observed. But this desirable feature makes them sensitive to confounding, which is the primary concern underlying classical randomized controlled trials. We highlight, for instance, that popular bandit algorithms cannot address the problem of identifying the best action when day-of-week effects may influence reward observations. In response, this paper proposes deconfounded Thompson sampling, which makes simple, but critical, modifications to the way Thompson sampling is usually applied. Theoretical guarantees suggest the algorithm strikes a delicate balance between adaptivity and robustness to confounding. It attains asymptotic lower bounds on the number of samples required to confidently identify the best action -- suggesting optimal adaptivity -- but also satisfies strong performance guarantees in the presence of day-of-week effects and delayed observations -- suggesting unusual robustness. At the core of the paper is a new model of contextual bandit experiments in which issues of delayed learning and distribution shift arise organically. Reference https://arxiv.org/pdf/2202.09036.pdf\nBio  Daniel Russo is an Associate Professor in the Decision, Risk, and Operations division of Columbia Business School. His research focuses on problems at the intersection of sequential decision-making and statistical machine learning. He completed his PhD at Stanford under the supervision of Ben Van Roy. Recording Lecture Recording\n","date":"2022-03-03T00:00:00Z","image":"https://97hongjun.github.io/p/adaptive-confounding-bandit/adaptConfoundMAB_hu3087afefa5e449a07021b12ce50c7734_525919_120x120_fill_box_smart1_3.png","permalink":"https://97hongjun.github.io/p/adaptive-confounding-bandit/","title":"Adaptivity and Confounding in Multi-Armed Bandit Experiments"},{"content":"Presenter  Xiuyuan (Lucy) Lu\nResearch Scientist,\nDeepMind, Mountain View\nAbstract  Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We develop concepts and establish a regret bound that together offer principled guidance. The bound sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that demonstrate improvements in data efficiency. Reference https://arxiv.org/pdf/2103.04047.pdf\nBio  Xiuyuan Lu is a research scientist at DeepMind. She is interested in building data-efficient reinforcement learning agents. She obtained her PhD in 2020 from the Department of Management Science and Engineering at Stanford University. Recording  Lecture 1 Recording\nLecture 2 Recording ","date":"2021-04-20T00:00:00Z","image":"https://97hongjun.github.io/p/rl-bit-by-bit/rlBitByBit_hu144592e2756b5495c68e6b8ed972c0f0_57185_120x120_fill_box_smart1_3.png","permalink":"https://97hongjun.github.io/p/rl-bit-by-bit/","title":"Reinforcement Learning, Bit by Bit"},{"content":"Presenter  Tengyu Ma\nAssistant Professor of Computer Science and Statistics,\nStanford\nAbstract  Deep model-based reinforcement learning methods have achieved state-of-the-art sample efficiency but we lack a theoretical understanding of them. This talk will first show that convergence to a global maximum requires an exponential number of samples even for a one-layer neural net bandit problem, which is strictly easier than RL. Therefore, we propose to study convergence to local maxima. For both nonlinear bandit and RL, I will present a model-based algorithm, Virtual Ascent with Online Model Learner (ViOL), which provably converges to a local maximum with sample complexity that only depends on the sequential Rademacher complexity of the model class. Our results imply novel global or local regret bounds on several concrete settings such as linear bandit with finite or sparse model class, and two-layer neural net bandit. Reference https://arxiv.org/pdf/2102.04168.pdf\nBio  Tengyu Ma is an assistant professor of Computer Science and Statistics at Stanford University. He received his Ph.D. from Princeton University and B.E. from Tsinghua University. His research interests include topics in machine learning and algorithms, such as deep learning and its theory, non-convex optimization, deep reinforcement learning, representation learning, and high-dimensional statistics. He is a recipient of NIPS'16 best student paper award, COLT'18 best paper award, ACM Doctoral Dissertation Award Honorable Mention, and Sloan Fellowship. Recording  Recording\n","date":"2021-04-15T00:00:00Z","image":"https://97hongjun.github.io/p/provable-model-based-bandit-rl/modelBasedMABRL_hu1f8d3a4dc93687f4035c14f62bc91bca_173684_120x120_fill_box_smart1_3.png","permalink":"https://97hongjun.github.io/p/provable-model-based-bandit-rl/","title":"Provable Model-based Nonlinear Bandit and Reinforcement Learning"},{"content":"Presenter  Kuang Xu\nAssociate Professor of Operations, Information and Technology,\nStanford Graduate School of Business\nAbstract  I will discuss in this talk a new diffusion-asymptotic analysis for sequentially randomized experiments. Rather than taking sample size n to infinity while keeping the problem parameters fixed, we let the mean signal level scale to the order 1/\\sqrt{n} so as to preserve the difficulty of the learning task as n gets large. In this regime, we show that the behavior of a class of methods for sequential experimentation converges to a diffusion limit. This connection enables us to make sharp performance predictions and obtain new insights on the behavior of Thompson sampling. Our diffusion asymptotics also help resolve a discrepancy between the Θ(log(n)) regret predicted by the fixed-parameter, large-sample asymptotics on the one hand, and the Θ(\\sqrt{n}) regret from worst-case, finite-sample analysis on the other, suggesting that it is an appropriate asymptotic regime for understanding practical large-scale sequential experiments. Reference https://arxiv.org/pdf/2101.09855.pdf\nBio  Kuang Xu is an Associate Professor of Operations, Information and Technology at Stanford Graduate School of Business, and Associate Professor by courtesy with the Electrical Engineering Department, Stanford University. Born in Suzhou, China, he received the B.S. degree in Electrical Engineering (2009) from the University of Illinois at Urbana-Champaign, and the Ph.D. degree in Electrical Engineering and Computer Science (2014) from the Massachusetts Institute of Technology. His research primarily focuses on understanding fundamental properties and design principles of large-scale stochastic systems using tools from probability theory and optimization, with applications in queueing networks, healthcare, privacy and machine learning. He received First Place in the INFORMS George E. Nicholson Student Paper Competition (2011), the Best Paper Award, as well as the Kenneth C. Sevcik Outstanding Student Paper Award at ACM SIGMETRICS (2013), and the ACM SIGMETRICS Rising Star Research Award (2020). He currently serves as an Associate Editor for Operations Research. Recording  Recording\n","date":"2021-03-23T00:00:00Z","image":"https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/diffusionAsymptotics_hu882245f5d6e1b349cc830eab9c265e3e_189146_120x120_fill_box_smart1_3.png","permalink":"https://97hongjun.github.io/p/diffusion-asymptotic-sequential-exp/","title":"Diffusion Asymptotics for Sequential Experiments"},{"content":"Presenter  Tor Lattimore\nResearch Scientist,\nDeepmind, London\nAbstract  Tor will give a whirlwind tour of a series of recent papers on the information directed sampling algorithm for sequential decision-making. The results come in three flavours. First, generalising and applying the IDS algorithm to problems with a rich information structure such as convex bandits and partial monitoring. Second, showing a connection between the optimisation problem solved by IDS and the optimisation problem that determines the asymptotic lower bound for stochastic structured bandit problems. Third, showing a deep connection between IDS and the mirror descent framework for convex optimisation. Reference  https://arxiv.org/abs/2011.05944\nhttps://arxiv.org/abs/2009.12228\nhttps://arxiv.org/abs/1907.05772\nhttps://arxiv.org/abs/2006.00475\nhttps://arxiv.org/abs/2002.11182\nhttps://arxiv.org/abs/1902.00470\nhttps://arxiv.org/abs/1905.11817\n### Bio  Tor Lattimore is a research scientist at DeepMind working on the foundations of machine learning and especially decision-making. Before joining DeepMind he was an assistant professor at Indiana University and a postdoc at the University of Alberta. He obtained his PhD from the Australian National University under the supervision of Marcus Hutter in 2014. Recording  Lecture 1 Recording\nLecture 2 Recording\n","date":"2021-01-11T00:00:00Z","image":"https://97hongjun.github.io/p/lec-ids/lecturesIDS_hue53073c20931b7daa6e1c4f835307f2e_44058_120x120_fill_box_smart1_3.png","permalink":"https://97hongjun.github.io/p/lec-ids/","title":"Lectures on Information Directed Sampling"}]